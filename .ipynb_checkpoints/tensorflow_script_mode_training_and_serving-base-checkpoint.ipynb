{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow training and serving\n",
    "\n",
    "Script mode is a training script format for TensorFlow that lets you execute any TensorFlow training script in SageMaker with minimal modification. The [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk) handles transferring your script to a SageMaker training instance. On the training instance, SageMaker's native TensorFlow support sets up training-related environment variables and executes your training script. In this tutorial, we use the SageMaker Python SDK to launch a training job and deploy the trained model.\n",
    "\n",
    "Script mode supports training with a Python script, a Python module, or a shell script. In this example, we will show how easily you can train a SageMaker using TensorFlow 2.1 scripts with SageMaker Python SDK. In addition, this notebook demonstrates how to perform real time inference with the [SageMaker TensorFlow Serving container](https://github.com/aws/sagemaker-tensorflow-serving-container). The TensorFlow Serving container is the default inference method for script mode. For full documentation on the TensorFlow Serving container, please visit [here](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/deploying_tensorflow_serving.rst).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sagemaker notebook 설명\n",
    "<p>Sagemaker notebook은 완전 관리형 서비스로 컨테이너 기반으로 구성되어 있습니다. 사용자가 직접 컨테이너를 볼 수 없지만, 내부적으로는 아래와 같은 원리로 동작합니다. </p>\n",
    "<p><img src=\"./fig/fig00.png\" width=\"800\", height=\"80\"></p>\n",
    "\n",
    "- **S3 (Simple Storage Serivce)** : Object Storage로서 학습할 데이터 파일과 학습 결과인 model, checkpoint, tensorboard를 위한 event 파일, 로그 정보 등을 저장하는데 사용합니다.\n",
    "- **SageMaker Notebook** : 학습을 위한 스크립트 작성과 디버깅, 그리고 실제 학습을 수행하기 위한 Python을 개발하기 위한 환경을 제공합니다.\n",
    "- **Amazon Elastic Container Registry(ECR)** :  Docker 컨테이너 이미지를 손쉽게 저장, 관리 및 배포할 수 있게 해주는 완전관리형 Docker 컨테이너 레지스트리입니다. Sagemaker는 기본적인 컨테이너를 제공하기 때문에 별도 ECR에 컨테이너 이미지를 등록할 필요는 없습니다. 하지만, 별도의 학습 및 배포 환경이 필요한 경우 custom 컨테이너 이미지를 만들어서 ECR에 등록한 후 이 환경을 활용할 수 있습니다.\n",
    "\n",
    "<p>학습과 추론을 하는 hosting 서비스는 각각 다른 컨테이너 환경에서 수행할 수 있으며, 쉽게 다량으로 컨테이너 환경을 확장할 수 있으므로 다량의 학습과 hosting이 동시에 가능합니다.   \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the environment\n",
    "\n",
    "Let's start by setting up the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (20.3.3)\n",
      "\u001b[33mWARNING: --use-feature=2020-resolver no longer has any effect, since it is now the default dependency resolver in pip. This will become an error in pip 21.0.\u001b[0m\n",
      "Requirement already satisfied: tensorflow-datasets in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (3.2.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (1.14.0)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (0.3.2)\n",
      "Requirement already satisfied: promise in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (2.3)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (19.3.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (4.44.1)\n",
      "Requirement already satisfied: tensorflow-metadata in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (0.24.0)\n",
      "Requirement already satisfied: wrapt in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (1.12.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (1.18.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (3.13.0)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (0.18.2)\n",
      "Requirement already satisfied: absl-py in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (0.9.0)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (1.1.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow-datasets) (46.1.3.post20200330)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow-datasets) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow-datasets) (1.25.8)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-metadata->tensorflow-datasets) (1.52.0)\n",
      "Requirement already satisfied: sagemaker-experiments in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (0.1.24)\n",
      "Requirement already satisfied: boto3>=1.12.8 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from sagemaker-experiments) (1.14.60)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.60 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from boto3>=1.12.8->sagemaker-experiments) (1.17.60)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from boto3>=1.12.8->sagemaker-experiments) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from boto3>=1.12.8->sagemaker-experiments) (0.9.4)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.60->boto3>=1.12.8->sagemaker-experiments) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.60->boto3>=1.12.8->sagemaker-experiments) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.60->boto3>=1.12.8->sagemaker-experiments) (1.25.8)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.60->boto3>=1.12.8->sagemaker-experiments) (1.14.0)\n",
      "Requirement already satisfied: smdebug in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (0.9.4)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from smdebug) (1.18.1)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from smdebug) (20.3)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from smdebug) (3.13.0)\n",
      "Requirement already satisfied: boto3>=1.10.32 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from smdebug) (1.14.60)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from boto3>=1.10.32->smdebug) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from boto3>=1.10.32->smdebug) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.60 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from boto3>=1.10.32->smdebug) (1.17.60)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.60->boto3>=1.10.32->smdebug) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.60->boto3>=1.10.32->smdebug) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.60->boto3>=1.10.32->smdebug) (1.25.8)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from protobuf>=3.6.0->smdebug) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from protobuf>=3.6.0->smdebug) (46.1.3.post20200330)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from packaging->smdebug) (2.4.6)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install --upgrade pip\n",
    "# !{sys.executable} -m pip install tensorflow-gpu==2.1.0\n",
    "!{sys.executable} -m pip install tensorflow-datasets --use-feature=2020-resolver\n",
    "!{sys.executable} -m pip install sagemaker-experiments\n",
    "!{sys.executable} -m pip install smdebug\n",
    "# !{sys.executable} -m pip install grpcio==1.24.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sagemaker\n",
    "import boto3\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "from sagemaker.debugger import Rule, DebuggerHookConfig, TensorBoardOutputConfig, CollectionConfig, rule_configs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client('sagemaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a S3 bucket to hold data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it.\n"
     ]
    }
   ],
   "source": [
    "# create a s3 bucket to hold data, note that your account might already created a bucket with the same name\n",
    "account_id = sess.client('sts').get_caller_identity()[\"Account\"]\n",
    "data_bucket = 'sagemaker-experiments-{}-{}'.format(sess.region_name, account_id)\n",
    "bucket = 'sagemaker-{}-{}'.format(sess.region_name, account_id)\n",
    "prefix = 'image_segmentation/oxford_iiit_pet/3.2.0'\n",
    "\n",
    "try:\n",
    "    if sess.region_name == \"us-east-1\":\n",
    "        sess.client('s3').create_bucket(Bucket=data_bucket)\n",
    "    else:\n",
    "        sess.client('s3').create_bucket(Bucket=data_bucket, \n",
    "                                        CreateBucketConfiguration={'LocationConstraint': sess.region_name})\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>이번 학습에 사용할 이미지 데이터는 <strong><a href=\"https://www.robots.ox.ac.uk/~vgg/data/pets/\" target=\"_blank\" class ='btn-default'>Oxford-IIIT Pet Dataset</a></strong> 입니다. Oxford-IIIT Pet Dataset은 <strong>37</strong>개 다른 종의 개와 고양이 이미지를 각각 200장 씩 제공하고 있으며, Ground Truth 또한 Classification, Object Detection, Segmentation와 관련된 모든 정보가 있으나, 이번 학습에서는 37개 class에 대해 일부 이미지로 Classification 문제를 해결하기 위해 학습을 진행할 예정입니다.</p>\n",
    "<p><img src=\"./fig/pet_annotations.jpg\" width=\"700\", height=\"70\"></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/ec2-user/tensorflow_datasets/oxford_iiit_pet/3.2.0\n",
      "INFO:absl:Reusing dataset oxford_iiit_pet (/home/ec2-user/tensorflow_datasets/oxford_iiit_pet/3.2.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from /home/ec2-user/tensorflow_datasets/oxford_iiit_pet/3.2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='oxford_iiit_pet',\n",
      "    version=3.2.0,\n",
      "    description='The Oxford-IIIT pet dataset is a 37 category pet image dataset with roughly 200\n",
      "images for each class. The images have large variations in scale, pose and\n",
      "lighting. All images have an associated ground truth annotation of breed.',\n",
      "    homepage='http://www.robots.ox.ac.uk/~vgg/data/pets/',\n",
      "    features=FeaturesDict({\n",
      "        'file_name': Text(shape=(), dtype=tf.string),\n",
      "        'image': Image(shape=(None, None, 3), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=37),\n",
      "        'segmentation_mask': Image(shape=(None, None, 1), dtype=tf.uint8),\n",
      "        'species': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
      "    }),\n",
      "    total_num_examples=7349,\n",
      "    splits={\n",
      "        'test': 3669,\n",
      "        'train': 3680,\n",
      "    },\n",
      "    supervised_keys=('image', 'label'),\n",
      "    citation=\"\"\"@InProceedings{parkhi12a,\n",
      "      author       = \"Parkhi, O. M. and Vedaldi, A. and Zisserman, A. and Jawahar, C.~V.\",\n",
      "      title        = \"Cats and Dogs\",\n",
      "      booktitle    = \"IEEE Conference on Computer Vision and Pattern Recognition\",\n",
      "      year         = \"2012\",\n",
      "    }\"\"\",\n",
      "    redistribution_info=,\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)\n",
    "builder = tfds.builder('oxford_iiit_pet:3.*.*')\n",
    "info = builder.info\n",
    "print(info)\n",
    "# by setting register_checksums as True to pass the check\n",
    "config = tfds.download.DownloadConfig(register_checksums = True)\n",
    "builder.download_and_prepare(download_config=config)\n",
    "dataset = builder.as_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload dataset to S3\n",
    "\n",
    "Next, we'll upload the TFRecord datasets to S3 so that we can use it in training and batch transform jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../../tensorflow_datasets/oxford_iiit_pet/3.2.0/image.image.json to s3://sagemaker-experiments-ap-northeast-2-322537213286/image_segmentation/oxford_iiit_pet/3.2.0/image.image.json\n",
      "upload: ../../tensorflow_datasets/oxford_iiit_pet/3.2.0/dataset_info.json to s3://sagemaker-experiments-ap-northeast-2-322537213286/image_segmentation/oxford_iiit_pet/3.2.0/dataset_info.json\n",
      "upload: ../../tensorflow_datasets/oxford_iiit_pet/3.2.0/label.labels.txt to s3://sagemaker-experiments-ap-northeast-2-322537213286/image_segmentation/oxford_iiit_pet/3.2.0/label.labels.txt\n",
      "upload: ../../tensorflow_datasets/oxford_iiit_pet/3.2.0/oxford_iiit_pet-test.tfrecord-00003-of-00004 to s3://sagemaker-experiments-ap-northeast-2-322537213286/image_segmentation/oxford_iiit_pet/3.2.0/oxford_iiit_pet-test.tfrecord-00003-of-00004\n",
      "upload: ../../tensorflow_datasets/oxford_iiit_pet/3.2.0/oxford_iiit_pet-test.tfrecord-00000-of-00004 to s3://sagemaker-experiments-ap-northeast-2-322537213286/image_segmentation/oxford_iiit_pet/3.2.0/oxford_iiit_pet-test.tfrecord-00000-of-00004\n",
      "upload: ../../tensorflow_datasets/oxford_iiit_pet/3.2.0/oxford_iiit_pet-train.tfrecord-00000-of-00004 to s3://sagemaker-experiments-ap-northeast-2-322537213286/image_segmentation/oxford_iiit_pet/3.2.0/oxford_iiit_pet-train.tfrecord-00000-of-00004\n",
      "upload: ../../tensorflow_datasets/oxford_iiit_pet/3.2.0/segmentation_mask.image.json to s3://sagemaker-experiments-ap-northeast-2-322537213286/image_segmentation/oxford_iiit_pet/3.2.0/segmentation_mask.image.json\n",
      "upload: ../../tensorflow_datasets/oxford_iiit_pet/3.2.0/species.labels.txt to s3://sagemaker-experiments-ap-northeast-2-322537213286/image_segmentation/oxford_iiit_pet/3.2.0/species.labels.txt\n",
      "upload: ../../tensorflow_datasets/oxford_iiit_pet/3.2.0/oxford_iiit_pet-train.tfrecord-00003-of-00004 to s3://sagemaker-experiments-ap-northeast-2-322537213286/image_segmentation/oxford_iiit_pet/3.2.0/oxford_iiit_pet-train.tfrecord-00003-of-00004\n",
      "upload: ../../tensorflow_datasets/oxford_iiit_pet/3.2.0/oxford_iiit_pet-test.tfrecord-00002-of-00004 to s3://sagemaker-experiments-ap-northeast-2-322537213286/image_segmentation/oxford_iiit_pet/3.2.0/oxford_iiit_pet-test.tfrecord-00002-of-00004\n",
      "upload: ../../tensorflow_datasets/oxford_iiit_pet/3.2.0/oxford_iiit_pet-train.tfrecord-00002-of-00004 to s3://sagemaker-experiments-ap-northeast-2-322537213286/image_segmentation/oxford_iiit_pet/3.2.0/oxford_iiit_pet-train.tfrecord-00002-of-00004\n",
      "upload: ../../tensorflow_datasets/oxford_iiit_pet/3.2.0/oxford_iiit_pet-test.tfrecord-00001-of-00004 to s3://sagemaker-experiments-ap-northeast-2-322537213286/image_segmentation/oxford_iiit_pet/3.2.0/oxford_iiit_pet-test.tfrecord-00001-of-00004\n",
      "upload: ../../tensorflow_datasets/oxford_iiit_pet/3.2.0/oxford_iiit_pet-train.tfrecord-00001-of-00004 to s3://sagemaker-experiments-ap-northeast-2-322537213286/image_segmentation/oxford_iiit_pet/3.2.0/oxford_iiit_pet-train.tfrecord-00001-of-00004\n",
      "input spec: s3://sagemaker-experiments-ap-northeast-2-322537213286/image_segmentation/oxford_iiit_pet/3.2.0/\n"
     ]
    }
   ],
   "source": [
    "inputs = \"s3://{}/{}/\".format(data_bucket, prefix)\n",
    "!aws s3 cp /home/ec2-user/tensorflow_datasets/oxford_iiit_pet/3.2.0/ $inputs --recursive\n",
    "print('input spec: {}'.format(inputs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a script for training\n",
    "\n",
    "Here is the entire script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: cannot read infile: [Errno 2] No such file or directory: 'source_dir/image_segmentation.py'\r\n"
     ]
    }
   ],
   "source": [
    "# # TensorFlow 2.1 script\n",
    "!pygmentize 'source_dir/image_segmentation.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a training job using the `TensorFlow` estimator\n",
    "\n",
    "The `sagemaker.tensorflow.TensorFlow` estimator handles locating the script mode container, uploading your script to a S3 location and creating a SageMaker training job. Let's call out a couple important parameters here:\n",
    "\n",
    "* `py_version` is set to `'py3'` to indicate that we are using script mode since legacy mode supports only Python 2. Though Python 2 is deprecated soon, you can use script mode with Python 2 by setting `py_version` to `'py2'` and `script_mode` to `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Experiments\n",
    "- experiments를 관리하고 추적하는 기능 제공\n",
    "<center><img src=\"./fig/experiments_fig.png\" width=\"900\" height=\"700\"></center>\n",
    "\n",
    "\n",
    "- trial components : pre-processing jobs, training jobs, and batch transform jobsb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Track an Experiment\n",
    "- Experiment 정보를 기록하기 위해 tracker를 사용\n",
    "- 기존 trial components 를 로딩하거나(Tracker.load) 신규 trial component를 생성하는 방식으로 사용(Tracker.create)\n",
    "- 아래는 데이터셋을 업로드하는 S3 버킷의 URI와 데이터셋 관련 정보를 log로 남기는 예제임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset 위치\n",
    "inputs= 's3://{}/{}'.format(data_bucket, prefix)\n",
    "# inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an Experiment\n",
    "- The top level entity as a collection of trials that are observed, compared, and evaluated as a group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ExperimentName': 'experiments-v2', 'ExperimentArn': 'arn:aws:sagemaker:ap-northeast-2:322537213286:experiment/experiments-v2', 'DisplayName': 'experiments-v2', 'Description': 'Segmentation of skincare images', 'CreationTime': datetime.datetime(2020, 9, 8, 23, 57, 39, 332000, tzinfo=tzlocal()), 'CreatedBy': {}, 'LastModifiedTime': datetime.datetime(2020, 12, 25, 2, 34, 33, 797000, tzinfo=tzlocal()), 'LastModifiedBy': {}, 'ResponseMetadata': {'RequestId': '2b66d77b-9de5-425e-a859-6ee9eacf4487', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '2b66d77b-9de5-425e-a859-6ee9eacf4487', 'content-type': 'application/x-amz-json-1.1', 'content-length': '307', 'date': 'Mon, 04 Jan 2021 14:28:07 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"experiments-v2\" ## 원하는 experiment 이름으로 변경\n",
    "\n",
    "experiment_existed = True\n",
    "try:\n",
    "    experiment = sm.describe_experiment(ExperimentName=experiment_name)\n",
    "except:\n",
    "    experiment_existed = False\n",
    "\n",
    "if not experiment_existed:\n",
    "    experiment = Experiment.create(\n",
    "        experiment_name=experiment_name, \n",
    "        description=\"Segmentation of skincare images\", \n",
    "        sagemaker_boto_client=sm)\n",
    "print(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create  Trials\n",
    "- 각  trial는 다른 hyperparameters에 대해 학습하는 과정을 나타냅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_name = f\"{int(time.time())}-{experiment_name}\"\n",
    "    \n",
    "train_trial = Trial.create(\n",
    "    trial_name=trial_name, \n",
    "    experiment_name=experiment_name,\n",
    "    sagemaker_boto_client=sm,\n",
    ")\n",
    "\n",
    "with Tracker.create(display_name=\"dataset-info\", sagemaker_boto_client=sm) as tracker:\n",
    "    tracker.log_parameters({\n",
    "        \"dataset\": \"oxford_iiit_pet\",\n",
    "        \"resize\" : 128\n",
    "    })\n",
    "    # we can log the s3 uri to the dataset we just uploaded\n",
    "    tracker.log_input(name=\"oxford_iiit_pet/3.1.0\", media_type=\"s3/uri\", value=inputs)\n",
    "    \n",
    "# associate the proprocessing trial component with the current trial\n",
    "train_trial.add_trial_component(tracker.trial_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Debugger\n",
    "- Training job에서 캡쳐하는 tensor 데이터를 모니터링, 기록 및 분석하여 훈련의 가시성을 높이는 기능 제공\n",
    "- 2단계 프로세스로 동작하는 smdebug 라이브러리 활용\n",
    "  1. tensors(및 scalar) 저장 : 특정 순간의 training job 상태 정의하며, 이러한 tensor를 캡쳐하고 분석하기 위해 저장 가능한 라이브러리 제공\n",
    "  2. 분석 : 저장된 tensor는 사전 패키지로 제공되는 rules에 의해 캡처가 되어 조건에 따라 분석이 됨\n",
    "<center><img src=\"https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/images/how-debugger-works-4.png\" width=\"700\" height=\"500\"></center>\n",
    "- hook_config 정의 : https://github.com/awslabs/sagemaker-debugger/blob/master/docs/api.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec: \n",
      "s3://sagemaker-ap-northeast-2-322537213286/1609770489-imgsegmentation-training-job/Tensorboard  \n",
      "s3://sagemaker-ap-northeast-2-322537213286/1609770489-imgsegmentation-training-job/output/debug\n"
     ]
    }
   ],
   "source": [
    "## artifacts 위치\n",
    "training_job_name = \"{}-imgsegmentation-training-job\".format(int(time.time()))\n",
    "\n",
    "tensorboard_output= 's3://{}/{}/{}'.format(bucket, training_job_name, 'Tensorboard')\n",
    "debugger_output_path = 's3://{}/{}/output/debug'.format(bucket, training_job_name)\n",
    "print('input spec: \\n{}  \\n{}'.format(tensorboard_output,debugger_output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_config = DebuggerHookConfig(\n",
    "    s3_output_path=debugger_output_path,\n",
    "    hook_parameters={\n",
    "        \"save_interval\": \"40\"\n",
    "    },\n",
    "    collection_configs=[\n",
    "        CollectionConfig(\"weights\"),\n",
    "        CollectionConfig(\"biases\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Rules\n",
    "\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = [\n",
    "        Rule.sagemaker(rule_configs.vanishing_gradient()),\n",
    "        Rule.sagemaker(rule_configs.loss_not_decreasing()),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "        'OUTOUT_CHANNELS' : 3,\n",
    "        'RESIZE_WIDTH' : 128,\n",
    "        'RESIZE_HEIGHT' : 128,\n",
    "        'EPOCHS' : 35,\n",
    "        'VAL_SUBSPLITS': 5,\n",
    "        'BATCH_SIZE': 32,\n",
    "        'BUFFER_SIZE': 1000,\n",
    "        'DATASET_NAME': 'oxford_iiit_pet',\n",
    "        'SAVE_INTERVAL' : 3\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "estimator = TensorFlow(entry_point='image_segmentation_base.py',\n",
    "                       source_dir='source_dir',\n",
    "                       role=role,\n",
    "                       instance_count=1,\n",
    "                       volume_size=400,\n",
    "                       instance_type='ml.p3.16xlarge',  # local_mode 수행 시 주석 처리\n",
    "#                        train_instance_type='local',  # local_mode 수행 시 사용\n",
    "#                        use_spot_instances=True,  # spot instance 활용\n",
    "                       max_run=12*60*60,         # spot instance 활용\n",
    "#                        max_wait=12*60*60,        # spot instance 활용\n",
    "                       framework_version='2.1.0',\n",
    "                       py_version='py3',\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       tensorboard_output_config=TensorBoardOutputConfig(tensorboard_output),\n",
    "                       rules = rules,\n",
    "                       debugger_hook_config=hook_config,\n",
    "                       metric_definitions=[\n",
    "                             {'Name': 'train:loss', 'Regex': 'Train loss: ([0-9\\\\.]+)'},\n",
    "                             {'Name': 'train:accuracy', 'Regex': 'Val loss: ([0-9\\\\.]+)'}\n",
    "                       ],\n",
    "                       enable_sagemaker_metrics=True\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling ``fit``\n",
    "\n",
    "To start a training job, we call `estimator.fit(training_data_uri)`.\n",
    "\n",
    "An S3 location is used here as the input. `fit` creates a default channel named `'training'`, which points to this S3 location. In the training script we can then access the training data from the location stored in `SM_CHANNEL_TRAINING`. `fit` accepts a couple other types of input as well. See the API doc [here](https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.EstimatorBase.fit) for details.\n",
    "\n",
    "When training is complete, the training job will upload the saved model for TensorFlow serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "INFO:sagemaker:Creating training-job with name: 1609770489-imgsegmentation-training-job\n"
     ]
    },
    {
     "ename": "ResourceLimitExceeded",
     "evalue": "An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.p3dn.24xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-76849ca20dbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;34m\"TrialComponentDisplayName\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         },\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/tensorflow/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config, run_tensorboard_locally)\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m             \u001b[0mfit_super\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/tensorflow/estimator.py\u001b[0m in \u001b[0;36mfit_super\u001b[0;34m()\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfit_super\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorFlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_tensorboard_locally\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mwait\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1089\u001b[0m             \u001b[0mtrain_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"enable_sagemaker_metrics\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_sagemaker_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image, algorithm_arn, encrypt_inter_container_traffic, train_use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     def process(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    336\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.p3dn.24xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit."
     ]
    }
   ],
   "source": [
    "estimator.fit(\n",
    "    inputs = {'training': inputs},\n",
    "    job_name=training_job_name,\n",
    "    logs='All',\n",
    "    experiment_config={\n",
    "            \"TrialName\": train_trial.trial_name,\n",
    "            \"TrialComponentDisplayName\": \"Training\",\n",
    "        },\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.logs_for_job(estimator.latest_training_job.name, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training is complete, it is always a good idea to take a look at training curves to diagnose problems, if any, during training and determine the representativeness of the training and validation datasets. We can do this with TensorBoard, and also with the Keras API: conveniently, the Keras fit invocation returns a data structure with the training history. In our training script, this history is saved on the lead training node, then uploaded with the model when training is complete.\n",
    "\n",
    "To retrieve the history, we first download the model locally, then unzip it to gain access to the history data structure. We can then simply load the history as JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts_dir = estimator.model_data.replace('model.tar.gz', '')\n",
    "print(artifacts_dir)\n",
    "!aws s3 ls --human-readable {artifacts_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./artifacts/\n",
    "!rm -rf ./models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json , os\n",
    "\n",
    "path = './models'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "!aws s3 cp {artifacts_dir}model.tar.gz {path}/model.tar.gz\n",
    "!tar -xzf {path}/model.tar.gz -C {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json , os\n",
    "\n",
    "path = './artifacts'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "!aws s3 cp {artifacts_dir}output.tar.gz {path}/output.tar.gz\n",
    "!tar -xzf {path}/output.tar.gz -C {path}\n",
    "\n",
    "with open(os.path.join(path, 'model_history.p'), \"r\") as f:\n",
    "    model_history = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the history with two graphs, one for accuracy and another for loss. Each graph shows the results for both the training and validation datasets. Although training is a stochastic process that can vary significantly between training jobs, overall you are likely to see that the training curves are converging smoothly and steadily to higher accuracy and lower loss, while the validation curves are more jagged. This is due to the validation dataset being relatively small and thus not as representative as the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_curves(history): \n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharex=True)\n",
    "    ax = axes[0]\n",
    "    ax.plot(history['accuracy'], label='train')\n",
    "    ax.plot(history['val_accuracy'], label='validation')\n",
    "    ax.set(\n",
    "        title='model accuracy',\n",
    "        ylabel='accuracy',\n",
    "        xlabel='epoch')\n",
    "    ax.legend()\n",
    "\n",
    "    ax = axes[1]\n",
    "    ax.plot(history['loss'], label='train')\n",
    "    ax.plot(history['val_loss'], label='validation')\n",
    "    ax.set(\n",
    "        title='model loss',\n",
    "        ylabel='loss',\n",
    "        xlabel='epoch')\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    \n",
    "plot_training_curves(model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list, title):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    \n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(Image.open(display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "title = ['Input Image', 'True Mask' ,'Predicted Mask']\n",
    "\n",
    "if hyperparameters['EPOCHS']%hyperparameters['SAVE_INTERVAL'] == 0:\n",
    "    err = hyperparameters['SAVE_INTERVAL']\n",
    "else:\n",
    "    err = hyperparameters['EPOCHS']%hyperparameters['SAVE_INTERVAL']\n",
    "\n",
    "last_epoch = hyperparameters['EPOCHS'] - err\n",
    "\n",
    "sample_image = os.path.join(path, 'sample_image.jpg')\n",
    "real_mask = os.path.join(path, 'sample_mask.png')\n",
    "predicted_mask = os.path.join(path, str(last_epoch) +'-predicted_mask.png')\n",
    "display([sample_image, real_mask, predicted_mask], title)\n",
    "\n",
    "\n",
    "predict_imgs = []\n",
    "titles = []\n",
    "\n",
    "for i in range(0,last_epoch+1,hyperparameters['SAVE_INTERVAL']):\n",
    "    predict_mask = os.path.join(path, str(i) +'-predicted_mask.png')\n",
    "    predict_imgs.append(predict_mask)\n",
    "    titles.append('epoch -' +str(i))\n",
    "display(predict_imgs, titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "description = sm.describe_training_job(TrainingJobName=estimator.latest_training_job.name)\n",
    "# description = sm.describe_training_job(TrainingJobName=job_name)\n",
    "print('Debug Hook configuration: ')\n",
    "print(description['DebugHookConfig'])\n",
    "print()\n",
    "print('Debug rules configuration: ')\n",
    "print(description['DebugRuleConfigurations'])\n",
    "print()\n",
    "print('Training job status')\n",
    "print(description['TrainingJobStatus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_sess = sagemaker.Session()\n",
    "sm_sess.logs_for_job(estimator.latest_training_job.name, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "iterate = True\n",
    "while(iterate):\n",
    "    description = sm.describe_training_job(TrainingJobName=estimator.latest_training_job.name)\n",
    "    eval_status_1 = description['DebugRuleEvaluationStatuses'][0]\n",
    "    eval_status_2 = description['DebugRuleEvaluationStatuses'][1]\n",
    "    print(eval_status_1)\n",
    "    print(eval_status_2)\n",
    "    if eval_status_1['RuleEvaluationStatus'] != 'InProgress' or eval_status_2['RuleEvaluationStatus'] != 'InProgress':\n",
    "        iterate = False\n",
    "    else:\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_arn = eval_status_1['RuleEvaluationJobArn']\n",
    "processing_job_name = processing_job_arn[processing_job_arn.rfind('/') + 1 :]\n",
    "print(processing_job_name)\n",
    "\n",
    "client = sm_sess.sagemaker_client\n",
    "descr = client.describe_processing_job(ProcessingJobName=processing_job_name)\n",
    "descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors_path = estimator.latest_job_debugger_artifacts_path()\n",
    "\n",
    "import smdebug.trials as smd\n",
    "trial = smd.create_trial(path=tensors_path)\n",
    "print(f\"Saved these tensors: {trial.tensor_names()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loss values during evaluation were {trial.tensor('val_accuracy').values()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Deploy the trained model to an endpoint\n",
    "\n",
    "The `deploy()` method creates a SageMaker model, which is then deployed to an endpoint to serve prediction requests in real time. We will use the TensorFlow Serving container for the endpoint, because we trained with script mode. This serving container runs an implementation of a web server that is compatible with SageMaker hosting protocol. The [Using your own inference code]() document explains how SageMaker runs inference containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, \n",
    "                             instance_type='ml.t2.medium', \n",
    "                             endpoint_name=training_job_name + '-t2medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke the endpoint (without inference.py)\n",
    "\n",
    "Let's download the training data and use that as input for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_handler(img_path):\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    sample_img = cv2.imread(img_path)\n",
    "    sample_img = cv2.resize(sample_img, dsize=(128, 128), interpolation=cv2.INTER_CUBIC)\n",
    "    sample_img = np.float32(sample_img)\n",
    "    sample_img = np.expand_dims(sample_img,axis=0)\n",
    "    sample_img = sample_img / 255.0 \n",
    "    return sample_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_handler(pred_mask):\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    pred_mask=np.array(pred_mask)\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    pred_mask = pred_mask[0]\n",
    "    pred_mask = tf.keras.preprocessing.image.array_to_img(pred_mask)\n",
    "    return pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/home/ec2-user/tensorflow_datasets/downloads/extracted/TAR.robots.ox.ac.uk_vgg_pets_imagesZxlcXhwB8atfm2pdIrjCelgNiW7ORYkX5h1Fkzf6MY0.tar.gz/images/Abyssinian_1.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "sample_img = input_handler(img_path)\n",
    "predictions = predictor.predict(sample_img)\n",
    "pred_mask=output_handler(predictions['predictions'])\n",
    "plt.imshow(pred_mask)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formats of the input and the output data correspond directly to the request and response formats of the `Predict` method in the [TensorFlow Serving REST API](https://www.tensorflow.org/serving/api_rest). SageMaker's TensforFlow Serving endpoints can also accept additional input formats that are not part of the TensorFlow REST API, including the simplified JSON format, line-delimited JSON objects (\"jsons\" or \"jsonlines\"), and CSV data.\n",
    "\n",
    "In this example we are using a `numpy` array as input, which will be serialized into the simplified JSON format. In addtion, TensorFlow serving can also process multiple items at once as you can see in the following code. You can find the complete documentation on how to make predictions against a TensorFlow serving SageMaker endpoint [here](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/deploying_tensorflow_serving.rst#making-predictions-against-a-sagemaker-endpoint)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke the endpoint (with inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import io\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sagemaker.tensorflow.serving import Model, Predictor\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make custom-serving-image\n",
    "<p> 현재 tensorflow serving 이미지에는 opencv를 지원하지 않고 있습니다. 이에 따라 별도로 opencv가 설치된 custom serving container를 생성해야 합니다. 이를 위해 custom-serving-container 폴더에서 Dockerfile을 이용하여 custom container image를 생성합니다.</p>\n",
    "<p>이미지를 생성하기 전에 작업하시는 region의 ECR 서비스에서 리포지토리를 만들어야 합니다.\n",
    "저는 리포지토리를 tensorflow200-opencv341-inference-eia 이름으로 us-east-2에서 작업을 하는 것으로 가정하고 아래와 같이 수행하였습니다.</p>\n",
    "\n",
    "<p> 기본적으로 제공되는 <strong><a href=\"https://github.com/aws/deep-learning-containers/blob/master/available_images.md\" target=\"_blank\" class ='btn-default'>Docker image</a></strong>는 다음과 같습니다. </p>\n",
    "\n",
    "<p>아래는 docker와 <strong><a href=\"https://docs.aws.amazon.com/ko_kr/cli/latest/userguide/cli-chap-install.html\" target=\"_blank\" class ='btn-default'>aws cli</a></strong>가 동작하는 터미널 환경에서 작업을 수행하시기 바랍니다. </p>\n",
    "    \n",
    "<pre>\n",
    "<code>\n",
    "   - $(aws ecr get-login --no-include-email --registry-ids 763104351884 --region us-east-2) \n",
    "   - docker build -f Dockerfile.eia -t tensorflow200-opencv341-inference-eia:2.0.0-cpu . \n",
    "   - $(aws ecr get-login --no-include-email)\n",
    "   - docker image tag tensorflow200-opencv341-inference-eia:2.0.0-cpu [내계정].dkr.ecr.us-east-2.amazonaws.com/tensorflow200-opencv341-inference-eia:2.0.0-cpu \n",
    "   - docker push [내계정].dkr.ecr.us-east-2.amazonaws.com/tensorflow200-opencv341-inference-eia:2.0.0-cpu\n",
    "</code>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> 각 환경에 맞게 container_image와, training_job, model_path를 셋팅합니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_image = 'XXXXXXXXXX.dkr.ecr.us-east-2.amazonaws.com/tensorflow200-opencv341-inference-eia:2.0.0-cpu'\n",
    "training_job = '[TTTTTTTT]-SkinCare-training-job'\n",
    "model_path='s3://sagemaker-us-east-2-XXXXXXXXXXXXXX/[TTTTTTTT]-training-job/output/model.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Model과 endpoint를 아래와 같이 생성합니다. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(model_data=model_path, role=role, framework_version='2.0.0',  entry_point='inference.py', source_dir='./source_dir', image=container_image)\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.t2.medium',accelerator_type='ml.eia2.medium', endpoint_name=training_job + '-t2me-eia2-invoke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "accept_content_type='application/json'\n",
    "input_content_type='application/x-image'\n",
    "\n",
    "endpoint_name='1590218539-SkinCare-training-job-t2me-eia2-invoke'\n",
    "\n",
    "## 테스트할 이미지\n",
    "image_path='/home/ec2-user/tensorflow_datasets/downloads/extracted/TAR.robots.ox.ac.uk_vgg_pets_imagesZxlcXhwB8atfm2pdIrjCelgNiW7ORYkX5h1Fkzf6MY0.tar.gz/images/Abyssinian_1.jpg'\n",
    "\n",
    "with open(image_path, mode='rb') as file:\n",
    "    img = file.read()\n",
    "\n",
    "file_byte_string = base64.encodebytes(img).decode(\"utf-8\")\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Accept=accept_content_type,\n",
    "    ContentType=input_content_type,\n",
    "    Body=file_byte_string\n",
    ")\n",
    "\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "image= io.BytesIO(base64.decodebytes(result.encode('utf-8')))\n",
    "pred_mask = Image.open(image)\n",
    "\n",
    "plt.imshow(pred_mask)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete the endpoint\n",
    "\n",
    "Let's delete the endpoint we just created to prevent incurring any extra costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
