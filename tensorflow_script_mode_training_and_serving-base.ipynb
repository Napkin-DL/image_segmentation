{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow training and serving\n",
    "\n",
    "Script mode is a training script format for TensorFlow that lets you execute any TensorFlow training script in SageMaker with minimal modification. The [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk) handles transferring your script to a SageMaker training instance. On the training instance, SageMaker's native TensorFlow support sets up training-related environment variables and executes your training script. In this tutorial, we use the SageMaker Python SDK to launch a training job and deploy the trained model.\n",
    "\n",
    "Script mode supports training with a Python script, a Python module, or a shell script. In this example, we will show how easily you can train a SageMaker using TensorFlow 2.1 scripts with SageMaker Python SDK. In addition, this notebook demonstrates how to perform real time inference with the [SageMaker TensorFlow Serving container](https://github.com/aws/sagemaker-tensorflow-serving-container). The TensorFlow Serving container is the default inference method for script mode. For full documentation on the TensorFlow Serving container, please visit [here](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/deploying_tensorflow_serving.rst).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the environment\n",
    "\n",
    "Let's start by setting up the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install tensorflow-gpu==2.1.0\n",
    "# !{sys.executable} -m pip install tensorflow-datasets\n",
    "# !{sys.executable} -m pip install sagemaker-experiments\n",
    "# !{sys.executable} -m pip install smdebug\n",
    "# !{sys.executable} -m pip install grpcio==1.24.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sagemaker\n",
    "import boto3\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "from sagemaker.debugger import Rule, DebuggerHookConfig, TensorBoardOutputConfig, CollectionConfig, rule_configs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "print(\"Python\", python_version())\n",
    "print(\"Notebook tensorflow_version\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a S3 bucket to hold data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a s3 bucket to hold data, note that your account might already created a bucket with the same name\n",
    "account_id = sess.client('sts').get_caller_identity()[\"Account\"]\n",
    "data_bucket = 'sagemaker-experiments-{}-{}'.format(sess.region_name, account_id)\n",
    "bucket = 'sagemaker-{}-{}'.format(sess.region_name, account_id)\n",
    "prefix = 'image_segmentation/oxford_iiit_pet/3.1.0'\n",
    "\n",
    "try:\n",
    "    if sess.region_name == \"us-east-1\":\n",
    "        sess.client('s3').create_bucket(Bucket=data_bucket)\n",
    "    else:\n",
    "        sess.client('s3').create_bucket(Bucket=data_bucket, \n",
    "                                        CreateBucketConfiguration={'LocationConstraint': sess.region_name})\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)\n",
    "builder = tfds.builder('oxford_iiit_pet:3.*.*')\n",
    "info = builder.info\n",
    "print(info)\n",
    "# by setting register_checksums as True to pass the check\n",
    "config = tfds.download.DownloadConfig(register_checksums = True)\n",
    "builder.download_and_prepare(download_config=config)\n",
    "dataset = builder.as_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload dataset to S3\n",
    "\n",
    "Next, we'll upload the TFRecord datasets to S3 so that we can use it in training and batch transform jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp /home/ec2-user/tensorflow_datasets/ s3://{data_bucket}/{prefix}/ --recursive\n",
    "\n",
    "inputs = sagemaker.Session().upload_data(path='/home/ec2-user/tensorflow_datasets/', bucket=bucket, key_prefix=prefix)\n",
    "print('input spec: {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a script for training\n",
    "\n",
    "Here is the entire script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TensorFlow 2.1 script\n",
    "!pygmentize 'source_dir/image_segmentation.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a training job using the `TensorFlow` estimator\n",
    "\n",
    "The `sagemaker.tensorflow.TensorFlow` estimator handles locating the script mode container, uploading your script to a S3 location and creating a SageMaker training job. Let's call out a couple important parameters here:\n",
    "\n",
    "* `py_version` is set to `'py3'` to indicate that we are using script mode since legacy mode supports only Python 2. Though Python 2 is deprecated soon, you can use script mode with Python 2 by setting `py_version` to `'py2'` and `script_mode` to `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Experiments\n",
    "- experiments를 관리하고 추적하는 기능 제공\n",
    "<center><img src=\"./fig/experiments_fig.png\" width=\"900\" height=\"700\"></center>\n",
    "\n",
    "\n",
    "- trial components : pre-processing jobs, training jobs, and batch transform jobsb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Track an Experiment\n",
    "- Experiment 정보를 기록하기 위해 tracker를 사용\n",
    "- 기존 trial components 를 로딩하거나(Tracker.load) 신규 trial component를 생성하는 방식으로 사용(Tracker.create)\n",
    "- 아래는 데이터셋을 업로드하는 S3 버킷의 URI와 데이터셋 관련 정보를 log로 남기는 예제임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset 위치\n",
    "inputs= 's3://{}/{}'.format(data_bucket, prefix)\n",
    "# inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an Experiment\n",
    "- The top level entity as a collection of trials that are observed, compared, and evaluated as a group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"experiments-skincare-v2\" ## 원하는 experiment 이름으로 변경\n",
    "\n",
    "experiment_existed = True\n",
    "try:\n",
    "    experiment = sm.describe_experiment(ExperimentName=experiment_name)\n",
    "except:\n",
    "    experiment_existed = False\n",
    "\n",
    "if not experiment_existed:\n",
    "    experiment = Experiment.create(\n",
    "        experiment_name=experiment_name, \n",
    "        description=\"Segmentation of skincare images\", \n",
    "        sagemaker_boto_client=sm)\n",
    "print(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create  Trials\n",
    "- 각  trial는 다른 hyperparameters에 대해 학습하는 과정을 나타냅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_name = f\"{int(time.time())}-{experiment_name}\"\n",
    "    \n",
    "train_trial = Trial.create(\n",
    "    trial_name=trial_name, \n",
    "    experiment_name=experiment_name,\n",
    "    sagemaker_boto_client=sm,\n",
    ")\n",
    "\n",
    "with Tracker.create(display_name=\"dataset-info\", sagemaker_boto_client=sm) as tracker:\n",
    "    tracker.log_parameters({\n",
    "        \"dataset\": \"oxford_iiit_pet\",\n",
    "        \"resize\" : 128\n",
    "    })\n",
    "    # we can log the s3 uri to the dataset we just uploaded\n",
    "    tracker.log_input(name=\"oxford_iiit_pet/3.1.0\", media_type=\"s3/uri\", value=inputs)\n",
    "    \n",
    "# associate the proprocessing trial component with the current trial\n",
    "train_trial.add_trial_component(tracker.trial_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Debugger\n",
    "- Training job에서 캡쳐하는 tensor 데이터를 모니터링, 기록 및 분석하여 훈련의 가시성을 높이는 기능 제공\n",
    "- 2단계 프로세스로 동작하는 smdebug 라이브러리 활용\n",
    "  1. tensors(및 scalar) 저장 : 특정 순간의 training job 상태 정의하며, 이러한 tensor를 캡쳐하고 분석하기 위해 저장 가능한 라이브러리 제공\n",
    "  2. 분석 : 저장된 tensor는 사전 패키지로 제공되는 rules에 의해 캡처가 되어 조건에 따라 분석이 됨\n",
    "<center><img src=\"https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/images/how-debugger-works-4.png\" width=\"700\" height=\"500\"></center>\n",
    "- hook_config 정의 : https://github.com/awslabs/sagemaker-debugger/blob/master/docs/api.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## artifacts 위치\n",
    "training_job_name = \"{}-imgsegmentation-training-job\".format(int(time.time()))\n",
    "\n",
    "tensorboard_output= 's3://{}/{}/{}'.format(bucket, training_job_name, 'Tensorboard')\n",
    "debugger_output_path = 's3://{}/{}/output/debug'.format(bucket, training_job_name)\n",
    "print('input spec: \\n{}  \\n{}'.format(tensorboard_output,debugger_output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_config = DebuggerHookConfig(\n",
    "    s3_output_path=debugger_output_path,\n",
    "    hook_parameters={\n",
    "        \"save_interval\": \"40\"\n",
    "    },\n",
    "    collection_configs=[\n",
    "        CollectionConfig(\"weights\"),\n",
    "        CollectionConfig(\"biases\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Rules\n",
    "\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = [\n",
    "        Rule.sagemaker(rule_configs.vanishing_gradient()),\n",
    "        Rule.sagemaker(rule_configs.loss_not_decreasing()),\n",
    "        Rule.sagemaker(\n",
    "            rule_configs.exploding_tensor(),\n",
    "            # configure your rule if applicable\n",
    "            rule_parameters={\"tensor_regex\": \".*\"},\n",
    "            # specify collections to save for processing your rule\n",
    "            collections_to_save=[\n",
    "                CollectionConfig(name=\"weights\")\n",
    "            ],\n",
    "        )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "        'OUTOUT_CHANNELS' : 3,\n",
    "        'RESIZE_WIDTH' : 128,\n",
    "        'RESIZE_HEIGHT' : 128,\n",
    "        'EPOCHS' : 15,\n",
    "        'VAL_SUBSPLITS': 5,\n",
    "        'BATCH_SIZE': 32,\n",
    "        'BUFFER_SIZE': 1000,\n",
    "        'DATASET_NAME': 'oxford_iiit_pet',\n",
    "        'SAVE_INTERVAL' : 3\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "estimator = TensorFlow(entry_point='image_segmentation_base.py',\n",
    "                       source_dir='source_dir',\n",
    "                       role=role,\n",
    "                       train_instance_count=1,\n",
    "                       train_volume_size=400,\n",
    "                       train_instance_type='ml.p3.2xlarge',  # local_mode 수행 시 주석 처리\n",
    "#                        train_instance_type='local',  # local_mode 수행 시 사용\n",
    "                       train_use_spot_instances=True,  # spot instance 활용\n",
    "                       train_max_run=12*60*60,         # spot instance 활용\n",
    "                       train_max_wait=12*60*60,        # spot instance 활용\n",
    "                       framework_version='2.1.0',\n",
    "                       py_version='py3',\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       tensorboard_output_config=TensorBoardOutputConfig(tensorboard_output),\n",
    "                       rules = rules,\n",
    "                       debugger_hook_config=hook_config,\n",
    "                       metric_definitions=[\n",
    "                            {'Name':'train:loss', 'Regex':'Train Loss: (.*?);'},\n",
    "                            {'Name':'test:loss', 'Regex':'Test Average loss: (.*?),'},\n",
    "                            {'Name':'test:accuracy', 'Regex':'Test Accuracy: (.*?)%;'}\n",
    "                       ],\n",
    "                       enable_sagemaker_metrics=True\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling ``fit``\n",
    "\n",
    "To start a training job, we call `estimator.fit(training_data_uri)`.\n",
    "\n",
    "An S3 location is used here as the input. `fit` creates a default channel named `'training'`, which points to this S3 location. In the training script we can then access the training data from the location stored in `SM_CHANNEL_TRAINING`. `fit` accepts a couple other types of input as well. See the API doc [here](https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.EstimatorBase.fit) for details.\n",
    "\n",
    "When training is complete, the training job will upload the saved model for TensorFlow serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "estimator.fit(\n",
    "    inputs = {'training': inputs},\n",
    "    job_name=training_job_name,\n",
    "    logs='All',\n",
    "    experiment_config={\n",
    "            \"TrialName\": train_trial.trial_name,\n",
    "            \"TrialComponentDisplayName\": \"Training\",\n",
    "        },\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sagemaker_session.logs_for_job(estimator.latest_training_job.name, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training is complete, it is always a good idea to take a look at training curves to diagnose problems, if any, during training and determine the representativeness of the training and validation datasets. We can do this with TensorBoard, and also with the Keras API: conveniently, the Keras fit invocation returns a data structure with the training history. In our training script, this history is saved on the lead training node, then uploaded with the model when training is complete.\n",
    "\n",
    "To retrieve the history, we first download the model locally, then unzip it to gain access to the history data structure. We can then simply load the history as JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts_dir = estimator.model_data.replace('model.tar.gz', '')\n",
    "print(artifacts_dir)\n",
    "!aws s3 ls --human-readable {artifacts_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./artifacts/\n",
    "!rm -rf ./models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json , os\n",
    "\n",
    "path = './models'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "!aws s3 cp {artifacts_dir}model.tar.gz {path}/model.tar.gz\n",
    "!tar -xzf {path}/model.tar.gz -C {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json , os\n",
    "\n",
    "path = './artifacts'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "!aws s3 cp {artifacts_dir}output.tar.gz {path}/output.tar.gz\n",
    "!tar -xzf {path}/output.tar.gz -C {path}\n",
    "\n",
    "with open(os.path.join(path, 'model_history.p'), \"r\") as f:\n",
    "    model_history = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the history with two graphs, one for accuracy and another for loss. Each graph shows the results for both the training and validation datasets. Although training is a stochastic process that can vary significantly between training jobs, overall you are likely to see that the training curves are converging smoothly and steadily to higher accuracy and lower loss, while the validation curves are more jagged. This is due to the validation dataset being relatively small and thus not as representative as the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_curves(history): \n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharex=True)\n",
    "    ax = axes[0]\n",
    "    ax.plot(history['accuracy'], label='train')\n",
    "    ax.plot(history['val_accuracy'], label='validation')\n",
    "    ax.set(\n",
    "        title='model accuracy',\n",
    "        ylabel='accuracy',\n",
    "        xlabel='epoch')\n",
    "    ax.legend()\n",
    "\n",
    "    ax = axes[1]\n",
    "    ax.plot(history['loss'], label='train')\n",
    "    ax.plot(history['val_loss'], label='validation')\n",
    "    ax.set(\n",
    "        title='model loss',\n",
    "        ylabel='loss',\n",
    "        xlabel='epoch')\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    \n",
    "plot_training_curves(model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list, title):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    \n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(Image.open(display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "title = ['Input Image', 'True Mask' ,'Predicted Mask']\n",
    "\n",
    "if hyperparameters['EPOCHS']%hyperparameters['SAVE_INTERVAL'] == 0:\n",
    "    err = hyperparameters['SAVE_INTERVAL']\n",
    "else:\n",
    "    err = hyperparameters['EPOCHS']%hyperparameters['SAVE_INTERVAL']\n",
    "\n",
    "last_epoch = hyperparameters['EPOCHS'] - err\n",
    "\n",
    "sample_image = os.path.join(path, 'sample_image.jpg')\n",
    "real_mask = os.path.join(path, 'sample_mask.png')\n",
    "predicted_mask = os.path.join(path, str(last_epoch) +'-predicted_mask.png')\n",
    "display([sample_image, real_mask, predicted_mask], title)\n",
    "\n",
    "\n",
    "predict_imgs = []\n",
    "titles = []\n",
    "\n",
    "for i in range(0,last_epoch+1,hyperparameters['SAVE_INTERVAL']):\n",
    "    predict_mask = os.path.join(path, str(i) +'-predicted_mask.png')\n",
    "    predict_imgs.append(predict_mask)\n",
    "    titles.append('epoch -' +str(i))\n",
    "display(predict_imgs, titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_dir = './models/000000001/'\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "                                       tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "tflite_model = converter.convert()\n",
    "with open(saved_model_dir + 'model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "description = sm.describe_training_job(TrainingJobName=estimator.latest_training_job.name)\n",
    "# description = sm.describe_training_job(TrainingJobName=job_name)\n",
    "print('Debug Hook configuration: ')\n",
    "print(description['DebugHookConfig'])\n",
    "print()\n",
    "print('Debug rules configuration: ')\n",
    "print(description['DebugRuleConfigurations'])\n",
    "print()\n",
    "print('Training job status')\n",
    "print(description['TrainingJobStatus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_sess = sagemaker.Session()\n",
    "sm_sess.logs_for_job(estimator.latest_training_job.name, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "iterate = True\n",
    "while(iterate):\n",
    "    description = sm.describe_training_job(TrainingJobName=estimator.latest_training_job.name)\n",
    "    eval_status_1 = description['DebugRuleEvaluationStatuses'][0]\n",
    "    eval_status_2 = description['DebugRuleEvaluationStatuses'][1]\n",
    "    print(eval_status_1)\n",
    "    print(eval_status_2)\n",
    "    if eval_status_1['RuleEvaluationStatus'] != 'InProgress' or eval_status_2['RuleEvaluationStatus'] != 'InProgress':\n",
    "        iterate = False\n",
    "    else:\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_arn = eval_status_1['RuleEvaluationJobArn']\n",
    "processing_job_name = processing_job_arn[processing_job_arn.rfind('/') + 1 :]\n",
    "print(processing_job_name)\n",
    "\n",
    "client = sm_sess.sagemaker_client\n",
    "descr = client.describe_processing_job(ProcessingJobName=processing_job_name)\n",
    "descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_sess.logs_for_processing_job(descr['ProcessingJobName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_arn = eval_status_2['RuleEvaluationJobArn']\n",
    "processing_job_name = processing_job_arn[processing_job_arn.rfind('/') + 1 :]\n",
    "print(processing_job_name)\n",
    "\n",
    "client = sm_sess.sagemaker_client\n",
    "descr = client.describe_processing_job(ProcessingJobName=processing_job_name)\n",
    "descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_sess.logs_for_processing_job(descr['ProcessingJobName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors_path = estimator.latest_job_debugger_artifacts_path()\n",
    "\n",
    "import smdebug.trials as smd\n",
    "trial = smd.create_trial(path=tensors_path)\n",
    "print(f\"Saved these tensors: {trial.tensor_names()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loss values during evaluation were {trial.tensor('val_accuracy').values()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Deploy the trained model to an endpoint\n",
    "\n",
    "The `deploy()` method creates a SageMaker model, which is then deployed to an endpoint to serve prediction requests in real time. We will use the TensorFlow Serving container for the endpoint, because we trained with script mode. This serving container runs an implementation of a web server that is compatible with SageMaker hosting protocol. The [Using your own inference code]() document explains how SageMaker runs inference containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.t2.medium', endpoint_name=training_job_name + '-t2medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke the endpoint (without inference.py)\n",
    "\n",
    "Let's download the training data and use that as input for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_handler(img_path):\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    sample_img = cv2.imread(img_path)\n",
    "    sample_img = cv2.resize(sample_img, dsize=(128, 128), interpolation=cv2.INTER_CUBIC)\n",
    "    sample_img = np.float32(sample_img)\n",
    "    sample_img = np.expand_dims(sample_img,axis=0)\n",
    "    sample_img = sample_img / 255.0 \n",
    "    return sample_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_handler(pred_mask):\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    pred_mask=np.array(pred_mask)\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    pred_mask = pred_mask[0]\n",
    "    pred_mask = tf.keras.preprocessing.image.array_to_img(pred_mask)\n",
    "    return pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/home/ec2-user/tensorflow_datasets/downloads/extracted/TAR.robots.ox.ac.uk_vgg_pets_imagesZxlcXhwB8atfm2pdIrjCelgNiW7ORYkX5h1Fkzf6MY0.tar.gz/images/Abyssinian_1.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "sample_img = input_handler(img_path)\n",
    "predictions = predictor.predict(sample_img)\n",
    "pred_mask=output_handler(predictions['predictions'])\n",
    "plt.imshow(pred_mask)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formats of the input and the output data correspond directly to the request and response formats of the `Predict` method in the [TensorFlow Serving REST API](https://www.tensorflow.org/serving/api_rest). SageMaker's TensforFlow Serving endpoints can also accept additional input formats that are not part of the TensorFlow REST API, including the simplified JSON format, line-delimited JSON objects (\"jsons\" or \"jsonlines\"), and CSV data.\n",
    "\n",
    "In this example we are using a `numpy` array as input, which will be serialized into the simplified JSON format. In addtion, TensorFlow serving can also process multiple items at once as you can see in the following code. You can find the complete documentation on how to make predictions against a TensorFlow serving SageMaker endpoint [here](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/deploying_tensorflow_serving.rst#making-predictions-against-a-sagemaker-endpoint)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke the endpoint (with inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import io\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sagemaker.tensorflow.serving import Model, Predictor\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make custom-serving-image\n",
    "<p> 현재 tensorflow serving 이미지에는 opencv를 지원하지 않고 있습니다. 이에 따라 별도로 opencv가 설치된 custom serving container를 생성해야 합니다. 이를 위해 custom-serving-container 폴더에서 Dockerfile을 이용하여 custom container image를 생성합니다.</p>\n",
    "<p>이미지를 생성하기 전에 작업하시는 region의 ECR 서비스에서 리포지토리를 만들어야 합니다.\n",
    "저는 리포지토리를 tensorflow200-opencv341-inference-eia 이름으로 us-east-2에서 작업을 하는 것으로 가정하고 아래와 같이 수행하였습니다.</p>\n",
    "\n",
    "<p>아래는 docker와 <strong><a href=\"https://docs.aws.amazon.com/ko_kr/cli/latest/userguide/cli-chap-install.html\" target=\"_blank\" class ='btn-default'>aws cli</a></strong>가 동작하는 터미널 환경에서 작업을 수행하시기 바랍니다. </p>\n",
    "    \n",
    "<pre>\n",
    "<code>\n",
    "   - $(aws ecr get-login --no-include-email --registry-ids 763104351884 --region us-east-2) \n",
    "   - docker build -f Dockerfile.eia -t tensorflow200-opencv341-inference-eia:2.0.0-cpu . \n",
    "   - $(aws ecr get-login --no-include-email)\n",
    "   - docker image tag tensorflow200-opencv341-inference-eia:2.0.0-cpu [내계정].dkr.ecr.us-east-2.amazonaws.com/tensorflow200-opencv341-inference-eia:2.0.0-cpu \n",
    "   - docker push [내계정].dkr.ecr.us-east-2.amazonaws.com/tensorflow200-opencv341-inference-eia:2.0.0-cpu\n",
    "</code>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> 각 환경에 맞게 container_image와, training_job, model_path를 셋팅합니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_image = 'XXXXXXXXXX.dkr.ecr.us-east-2.amazonaws.com/tensorflow200-opencv341-inference-eia:2.0.0-cpu'\n",
    "training_job = '[TTTTTTTT]-SkinCare-training-job'\n",
    "model_path='s3://sagemaker-us-east-2-XXXXXXXXXXXXXX/[TTTTTTTT]-training-job/output/model.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Model과 endpoint를 아래와 같이 생성합니다. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(model_data=model_path, role=role, framework_version='2.0.0',  entry_point='inference.py', source_dir='./source_dir', image=container_image)\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.t2.medium',accelerator_type='ml.eia2.medium', endpoint_name=training_job + '-t2me-eia2-invoke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "accept_content_type='application/json'\n",
    "input_content_type='application/x-image'\n",
    "\n",
    "endpoint_name='1590218539-SkinCare-training-job-t2me-eia2-invoke'\n",
    "\n",
    "## 테스트할 이미지\n",
    "image_path='/home/ec2-user/tensorflow_datasets/downloads/extracted/TAR.robots.ox.ac.uk_vgg_pets_imagesZxlcXhwB8atfm2pdIrjCelgNiW7ORYkX5h1Fkzf6MY0.tar.gz/images/Abyssinian_1.jpg'\n",
    "\n",
    "with open(image_path, mode='rb') as file:\n",
    "    img = file.read()\n",
    "\n",
    "file_byte_string = base64.encodebytes(img).decode(\"utf-8\")\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Accept=accept_content_type,\n",
    "    ContentType=input_content_type,\n",
    "    Body=file_byte_string\n",
    ")\n",
    "\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "image= io.BytesIO(base64.decodebytes(result.encode('utf-8')))\n",
    "pred_mask = Image.open(image)\n",
    "\n",
    "plt.imshow(pred_mask)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete the endpoint\n",
    "\n",
    "Let's delete the endpoint we just created to prevent incurring any extra costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
